{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import habitat\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import init_config, FrameSkip, FrameStack, draw_top_down_map, RewardForwardFilter, RunningMeanStd,make_train_data\n",
    "from agent import RNDAgent\n",
    "from env import SimpleRLEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = init_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_basic = SimpleRLEnv(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMESKIP = 2\n",
    "FRAMESTACK = 4\n",
    "env = FrameSkip(env_basic, skip=FRAMESKIP)\n",
    "env = FrameStack(env, FRAMESTACK, channel_order='chw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "positions = []\n",
    "rotations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,rew,done,info = env.step(2)\n",
    "top_down_map = draw_top_down_map(info, env.env.env.obs[\"heading\"][0], env.env.env.obs['rgb'].shape[0]) \n",
    "plt.imshow(top_down_map)\n",
    "print(done, rew)\n",
    "print(env.env.env.trux,env.env.env.truy)\n",
    "print(env.env.env.goalx, env.env.env.goaly)\n",
    "print(env.env.env.state['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.env.env.trux,env.env.env.truy)\n",
    "print(env.env.env.goalx, env.env.env.goaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMESTACK = FRAMESTACK\n",
    "output_size = 4\n",
    "num_worker = 1\n",
    "num_step = 500\n",
    "gamma = 0.999\n",
    "lam = 0.95\n",
    "learning_rate = 1e-4\n",
    "entropy_coef = 0.001\n",
    "clip_grad_norm = 0.5\n",
    "epoch = 100000000\n",
    "mini_batch = 100\n",
    "batch_size = int(num_step * num_worker / mini_batch)\n",
    "ppo_eps = 0.1\n",
    "int_gamma = 0.99\n",
    "int_coef = 1.\n",
    "ext_coef = 2.\n",
    "use_cuda = True\n",
    "use_gae = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = RNDAgent(\n",
    "        FRAMESTACK,\n",
    "        output_size,\n",
    "        num_worker,\n",
    "        num_step,\n",
    "        gamma,\n",
    "        lam=lam,\n",
    "        learning_rate=learning_rate,\n",
    "        ent_coef=entropy_coef,\n",
    "        clip_grad_norm=clip_grad_norm,\n",
    "        epoch=epoch,\n",
    "        batch_size=batch_size,\n",
    "        ppo_eps=ppo_eps,\n",
    "        use_cuda=use_cuda,\n",
    "        use_gae=use_gae\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "global_update = 0\n",
    "sample_env_idx = 0\n",
    "sample_i_rall = 0\n",
    "sample_rall = 0\n",
    "sample_step = 0\n",
    "sample_episode = 0\n",
    "all_steps = 0\n",
    "rew_trees = []\n",
    "discounted_reward = RewardForwardFilter(int_gamma)\n",
    "reward_rms = RunningMeanStd()\n",
    "#wandb.watch(agent.model)\n",
    "#wandb.watch(agent.rnd)\n",
    "total_state_array, ext_target_array, int_target_array, total_action_array, total_adv_array, total_next_obs_array, total_policy_array = [], [], [], [], [], [], []\n",
    "\n",
    "while True:\n",
    "    states = env.reset()\n",
    "    total_state, total_reward, total_done, total_next_state, total_action, total_int_reward, total_next_obs, total_ext_values, total_int_values, total_policy, total_policy_np = \\\n",
    "        [], [], [], [], [], [], [], [], [], [], []\n",
    "    global_step += (num_worker * num_step)\n",
    "    global_update += 1\n",
    "    stps = 0\n",
    "    dne = False\n",
    "    \n",
    "    for _ in range(num_step):\n",
    "        actions, value_ext, value_int, policy = agent.get_action(states)\n",
    "        intrinsic_reward = agent.compute_intrinsic_reward(states)\n",
    "        s,r,d,i = env.step(actions[0])\n",
    "        stps+=1    \n",
    "        next_states, rewards, dones, real_dones, log_rewards, next_obs, intrinsic_rewards = [], [], [], [], [], [], []\n",
    "        next_states.append(s)\n",
    "        rewards.append(r)\n",
    "        dones.append(d)\n",
    "        next_obs.append(s)\n",
    "        intrinsic_rewards.append(intrinsic_reward)\n",
    "        \n",
    "        next_states = np.stack(next_states)\n",
    "        rewards = np.hstack(rewards)\n",
    "        dones = np.hstack(dones)\n",
    "        next_obs = np.stack(next_obs)\n",
    "        intrinsic_rewards = np.hstack(intrinsic_reward)\n",
    "        \n",
    "        \n",
    "        sample_i_rall += intrinsic_reward[sample_env_idx]\n",
    "        total_next_obs.append(next_obs)\n",
    "        total_int_reward.append(intrinsic_reward)\n",
    "        total_state.append(states)\n",
    "        total_reward.append(rewards)\n",
    "        total_done.append(dones)\n",
    "        total_action.append(actions)\n",
    "        total_ext_values.append(value_ext)\n",
    "        total_int_values.append(value_int)\n",
    "        total_policy.append(policy)\n",
    "        total_policy_np.append(policy.cpu().numpy())\n",
    "        \n",
    "        states = next_states[0]\n",
    "        sample_rall += r\n",
    "        sample_step += 1\n",
    "        \n",
    "        if d:\n",
    "            dne = True\n",
    "            sample_episode += 1\n",
    "            sample_rall = 0\n",
    "            \n",
    "            sample_i_rall = 0\n",
    "            \n",
    "            break\n",
    "    \n",
    "    if not dne:\n",
    "        sample_episode += 1\n",
    "        sample_rall = 0\n",
    "        sample_step = 0\n",
    "        sample_i_rall = 0\n",
    "        \n",
    "    \n",
    "    rew_trees.append(sum(sum(total_reward)))\n",
    "    all_steps += sample_step\n",
    "    print('\\t',sample_step,rew_trees[-1],np.mean(rew_trees[-100:]),i['spl'],all_steps)\n",
    "    #wandb.log({\"Steps in one run\": sample_step, \"Episode reward\": rew_trees[-1], \"Episode SPL\": i['spl'], \"All steps\": all_steps})\n",
    "    sample_step = 0\n",
    "    _, value_ext, value_int, _ = agent.get_action(states)\n",
    "    total_ext_values.append(value_ext)\n",
    "    total_int_values.append(value_int) \n",
    "    \n",
    "    total_state = np.stack(total_state)\n",
    "    total_reward = np.stack(total_reward).transpose().clip(-1, 1)\n",
    "    total_action = np.stack(total_action).transpose().reshape([-1])\n",
    "    total_done = np.stack(total_done).transpose()\n",
    "    total_next_obs = np.stack(total_next_obs)\n",
    "    total_ext_values = np.stack(total_ext_values).transpose()\n",
    "    total_int_values = np.stack(total_int_values).transpose()\n",
    "    total_logging_policy = np.vstack(total_policy_np)\n",
    "    \n",
    "    total_int_reward = np.stack(total_int_reward).transpose()\n",
    "    total_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in\n",
    "                                     total_int_reward.T])\n",
    "    mean, std, count = np.mean(total_reward_per_env), np.std(total_reward_per_env), len(total_reward_per_env)\n",
    "    reward_rms.update_from_moments(mean, std ** 2, count)\n",
    "    total_int_reward /= np.sqrt(reward_rms.var)\n",
    "    \n",
    "    ext_target, ext_adv = make_train_data(total_reward,\n",
    "                                          total_done,\n",
    "                                          total_ext_values.reshape((1,-1)),\n",
    "                                          gamma,\n",
    "                                          stps,\n",
    "                                          num_worker,\n",
    "                                          use_gae,\n",
    "                                          lam)\n",
    "    # None Episodic\n",
    "    int_target, int_adv = make_train_data(total_int_reward,\n",
    "                                          np.zeros_like(total_int_reward),\n",
    "                                          total_int_values.reshape((1,-1)),\n",
    "                                          int_gamma,\n",
    "                                          stps,\n",
    "                                          num_worker,\n",
    "                                          use_gae,\n",
    "                                          lam)\n",
    "    \n",
    "    \n",
    "    total_adv = int_adv * int_coef + ext_adv * ext_coef\n",
    "    print(global_update,'\\t',sum(total_adv),sum(ext_adv),sum(int_adv))\n",
    "    #wandb.log({\"Episode Total Reward\": sum(total_adv), \"Episode Ext Reward\": sum(ext_adv), \"Episode Int Reward\": sum(int_adv)})\n",
    "    st = time.time()\n",
    "    \n",
    "    \n",
    "    \n",
    "    total_state_array.extend(total_state)\n",
    "    ext_target_array.extend(ext_target)\n",
    "    int_target_array.extend(int_target)\n",
    "    total_action_array.extend(total_action)\n",
    "    total_adv_array.extend(total_adv)\n",
    "    total_next_obs_array.extend(total_next_obs)\n",
    "    total_policy_array.extend(total_policy)\n",
    "    \n",
    "    if len(total_state_array)>400:\n",
    "        agent.train_model(total_state_array, ext_target_array, int_target_array, total_action_array,\n",
    "                      total_adv_array, total_next_obs_array,\n",
    "                      total_policy_array)\n",
    "        total_state_array, ext_target_array, int_target_array, total_action_array, total_adv_array, total_next_obs_array, total_policy_array = [], [], [], [], [], [], []\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Time to train: \",end-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
